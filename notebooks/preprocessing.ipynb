{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stops_nltk = set(stopwords.words(\"english\"))\n",
    "stops_spacy = STOP_WORDS.union({'ll', 've', 'pron','okay','oh','like','know','yeah','yea','yep',\\\n",
    "                                \"like like\",\"oh like\",\"yeah like\",\"yeah yeah\",\"oh okay\",\"wow\"})\n",
    "stops = stops_nltk.union(stops_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "global df, show_descriptions\n",
    "\n",
    "meta_data = []\n",
    "with open(\"../data/metadata.tsv\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile,delimiter=\"\\t\")\n",
    "    for row in csvreader:\n",
    "        meta_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(meta_data[1:],columns=meta_data[0])\n",
    "show_filename_prefixes = df.show_filename_prefix\n",
    "episode_filename_prefixes = df.episode_filename_prefix\n",
    "show_names = df.show_name\n",
    "episode_names = df.episode_name\n",
    "show_duration = df.duration\n",
    "publisher_name = df.publisher\n",
    "\n",
    "show_descriptions = {}\n",
    "\n",
    "for item1,item2 in zip(list(show_names),list(df.show_description)):\n",
    "    show_descriptions[item1] = item2.strip()\n",
    "    \n",
    "episode_descriptions = {}\n",
    "for item1,item2 in zip(list(episode_filename_prefixes),list(df.episode_description)):\n",
    "    episode_descriptions[item1] = item2.strip()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_topics = [\"comedy\",\"news\",\"crime\",\"science\",\"economics\",\"politics\",\"education\",\\\n",
    "                  \"sports\",\"lifestyle\",\"health\",\"wellbeing\",\"religion\",\"faith\",\"music\",\\\n",
    "                  \"art\",\"fashion\",\"literature\",\"humanities\",\"drama\",\"fitness\",\"drama\",\\\n",
    "                  \"fantasy\",\"scifi\",\"gameshow\",\"news quiz\",\"games\",\"mental health\",\\\n",
    "                  \"humor\",\"research\",\"technology\",\"society\",\"social\",\"culture\",\"lifestyle\",\\\n",
    "                  \"songs\",\"cooking\",\"culinary\",\"food\",\"travel\",\"films\",\"movies\",\"tv\",\"tv shows\",\\\n",
    "                  \"climate\",\"space\",\"planet\",\"digital\",\"artificial intelligence\", \"ai\",\\\n",
    "                  \"cars\",\"car\",\"nutrition\",\"wellness\",\"family\",\"history\",\"geography\",\"physics\",\\\n",
    "                  \"mathematics\",\"math\",\"chemistry\",\"biology\",\"documentary\",\"commentary\",\"nfl\",\\\n",
    "                  \"mls\",\"nba\",\"mlb\",\"stocks\",\"stock\",\"market\",\"wall street\",\"business\",\\\n",
    "                  \"reality shows\",\"investing\",\"social media\",\"biography\",\"biographies\",\\\n",
    "                  \"data science\",\"medicine\",\"media\",\"books\",\"book\",\"europe\",\"asia\",\"canada\",\\\n",
    "                  \"south america\",\"north america\",\"america\",\"usa\",\"facebook\",\"netflix\",\"google\"\\\n",
    "                  \"instagram\",\"tiktok\",\"amazon\",\"apple\",\"twitter\",\"adventure\",\"pets\",\"dogs\",\\\n",
    "                  \"cats\",\"dog\",\"cat\",\"nintendo\",\"xbox\",\"playstation\",\"ps4\",\"ps5\",\"theatre\",\"mars\"\\\n",
    "                  \"tennis\",\"australia\",\"conspiracy\",\"war\",\"epidemic\",\"pandemic\",\"climate change\"\\\n",
    "                  \"astrology\",\"novel\",\"church\",\"christ\",\"romance\",\"english\",\"kids\",\"astronomy\"\\\n",
    "                  \"design\"]\n",
    "\n",
    "formats = [\"monologue\",\"interview\",\"storytelling\",\"repurposed\",\\\n",
    "           \"bite-sized\",\"co-host conversation\",\"debate\",\"narrative\",\\\n",
    "           \"scripted\",\"improvised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(my_dict, val):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: my_dict is a dictionary\n",
    "           val is a value\n",
    "    \n",
    "    returns key of val in my_dict\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, value in my_dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "\n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "# # usual cleaning & remove urls and links\n",
    "def remove_stops(text,stops):\n",
    "    text = re.sub(r'www\\.[a-z0-9A-Z.]*', \"\", text)\n",
    "    text = re.sub(r'https://[a-z0-9A-Z.]*', \"\", text)\n",
    "    words = text.split()\n",
    "    final = []\n",
    "    for word in words:\n",
    "        if word not in stops:\n",
    "            final.append(word)\n",
    "    final = \" \".join(final)\n",
    "    final = final.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    final = \"\".join([item for item in final if not item.isdigit()])\n",
    "    while \"  \" in final:\n",
    "        final = final.replace(\"  \",\" \")\n",
    "    return final\n",
    "                       \n",
    "def clean_descriptions(docs):\n",
    "    stops = stopwords.words(\"english\")\n",
    "    final = []\n",
    "    for doc in docs:\n",
    "        clean_doc = remove_stops(doc, stops)\n",
    "        final.append(clean_doc)\n",
    "    return final\n",
    "\n",
    "# lemmatization\n",
    "def get_lemmatized(text):\n",
    "    for phrase in text:\n",
    "        for word in nlp(phrase):\n",
    "            if word.pos_ == \"VERB\":\n",
    "                print(word.lemma_)\n",
    "\n",
    "def get_named_entities(text):\n",
    "    return nlp(text.lower()).ents\n",
    "\n",
    "def get_noun_chunks(text):\n",
    "    non_stop_noun_chunks = []\n",
    "    stops = stopwords.words(\"english\")\n",
    "    for word in nlp(text.lower()).noun_chunks:\n",
    "        if str(word) not in stops:\n",
    "            non_stop_noun_chunks.append(word)\n",
    "    return non_stop_noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcasts_genres_topics = {}\n",
    "\n",
    "for k,show in enumerate(show_names):\n",
    "    \n",
    "    ner = get_named_entities(show.strip())\n",
    "    if len(ner)>0:\n",
    "        podcasts_genres_topics[(k,show)] = [str(item) for item in list(ner) if not str(item).isdigit()]\n",
    "        \n",
    "    noun_chunks = get_noun_chunks(show.strip())  \n",
    "    if noun_chunks:\n",
    "        try:\n",
    "            podcasts_genres_topics[(k,show)].extend([entry.strip() for entry in\\\n",
    "                                                     [re.sub(r'[^0-9a-zA-Z\\\" \"]+', '', str(item))\\\n",
    "                                                     for item in noun_chunks if not str(item).isdigit()]])\n",
    "        except KeyError as ke:\n",
    "            podcasts_genres_topics[(k,show)] = [entry.strip() for entry in\\\n",
    "                                                     [re.sub(r'[^0-9a-zA-Z\\\" \"]+', '', str(item))\\\n",
    "                                                     for item in noun_chunks if not str(item).isdigit()]]\n",
    "    \n",
    "    keywords = show.lower().split(\" \")\n",
    "    for word in keywords:\n",
    "        if word in genres_topics:\n",
    "            if (k,show) in podcasts_genres_topics:\n",
    "                if word not in podcasts_genres_topics[(k,show)]:\n",
    "                    podcasts_genres_topics[(k,show)].append(word)\n",
    "            else:\n",
    "                podcasts_genres_topics[(k,show)] = [word]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9239464692482916"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(podcasts_genres_topics)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"show_names_genres_topics.txt\",\"w+\") as f:\n",
    "    for key,val in podcasts_genres_topics.items():\n",
    "        f.write(str(key[0])+\", \")\n",
    "        f.write(key[1]+\", \")\n",
    "        for phrase in val:\n",
    "            f.write(phrase+\", \")\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = []\n",
    "# for genre in nlp(\" \".join(genres)):\n",
    "#     columns.append(genre.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_show_descriptions = {\"ADJ\":{},\"NOUN\":{},\"VERB\":{},\"NE\":{},\"NC\":{}}\n",
    "for (item1,item2) in list(show_descriptions.items()):\n",
    "    doc = nlp(item2)\n",
    "    pos_show_descriptions[\"NE\"][item1] = doc.ents\n",
    "    pos_show_descriptions[\"NC\"][item1] = doc.noun_chunks\n",
    "    for word in doc:\n",
    "        if word.pos_ == \"ADJ\" and str(word) not in stops:\n",
    "            if item1 not in pos_show_descriptions[\"ADJ\"]:\n",
    "                pos_show_descriptions[\"ADJ\"][item1] = [word.lemma_]\n",
    "            else:\n",
    "                pos_show_descriptions[\"ADJ\"][item1].append(word.lemma_)\n",
    "        if word.pos_ == \"VERB\" and str(word) not in stops:\n",
    "            if item1 not in pos_show_descriptions[\"VERB\"]:\n",
    "                pos_show_descriptions[\"VERB\"][item1] = [word.lemma_]\n",
    "            else:\n",
    "                pos_show_descriptions[\"VERB\"][item1].append(word.lemma_)\n",
    "        if word.pos_ == \"NOUN\" and str(word) not in stops:\n",
    "            if item1 not in pos_show_descriptions[\"NOUN\"]:\n",
    "                pos_show_descriptions[\"NOUN\"][item1] = [word.lemma_]\n",
    "            else:\n",
    "                pos_show_descriptions[\"NOUN\"][item1].append(word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows = df.groupby(by=['show_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions_aggregated = shows.apply(lambda x: list(x.episode_description)+[x.show_description.unique()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1073"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(val) for key,val in descriptions_aggregated.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hundred_or_more_episode_podcasts = [item1.strip() for (item1,item2) in descriptions_aggregated.items()\\\n",
    "                                if (len(item2) >= 100) and (len(item2) <= 1073)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# cmd = subprocess.Popen('pwd', stdout=subprocess.PIPE)\n",
    "# cmd_out, cmd_err = cmd.communicate()\n",
    "# local_path = os.fsdecode(cmd_out).strip() + \"/0/\"\n",
    "\n",
    "# transcript_files = {}\n",
    "\n",
    "# for folder1 in os.listdir(os.fsencode(local_path)):\n",
    "#     foldername1 = os.fsdecode(folder1)\n",
    "#     if len(foldername1) == 1:\n",
    "#         for folder2 in os.listdir(os.fsencode(local_path+foldername1)):\n",
    "#             foldername2 = os.fsdecode(folder2)\n",
    "#             if len(foldername2.split(\"_\")) == 2:\n",
    "#                 s = os.fsencode(local_path + \"/\" + foldername1 + \"/\" + foldername2 + \"/\")\n",
    "#                 for file in os.listdir(s):\n",
    "#                     transcript_files[os.fsdecode(s)] = os.fsdecode(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_ner = {}\n",
    "for item in ten_or_more_episode_podcasts:\n",
    "    tup = get_named_entities(item) \n",
    "    if len(tup) > 0:\n",
    "        topics_ner[item] = list(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_ner = {}\n",
    "for item in ten_or_more_episode_podcasts:\n",
    "    tup = get_named_entities(item) \n",
    "    if len(tup) > 0:\n",
    "        topics_ner[item] = list(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing interquartile range\n",
    "\n",
    "# N = len(arr)\n",
    "# if N%2 == 0:\n",
    "#     m = (N//2)\n",
    "#     a = arr[m:]\n",
    "#     if m%2 == 0:\n",
    "#         Q1 = 0.5*(arr[m//2] + arr[(m//2)-1])\n",
    "#         Q3 = 0.5*(a[m//2] + a[(m//2)-1])\n",
    "#     else:\n",
    "#         Q1 = arr[m//2]\n",
    "#         Q3 = a[m//2]\n",
    "# else:\n",
    "#     m = (N-1)//2\n",
    "#     a = arr[m+1:]\n",
    "#     if m%2 == 0:\n",
    "#         Q1 = 0.5*(arr[m//2] + arr[(m//2)-1])\n",
    "#         Q3 = 0.5*(a[m//2] + a[(m//2)-1])\n",
    "#     else:\n",
    "#         Q1 = arr[m//2]\n",
    "#         Q3 = a[m//2]\n",
    "\n",
    "# print(Q3-Q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textacy\n",
    "\n",
    "# patterns = [{\"POS\":\"ADV\"},{\"POS\":\"VERB\"}]\n",
    "\n",
    "# verb_phrases = textacy.extract.matches.token_matches(nlp(transcripts[key[0]].lower()), pattern = patterns)\n",
    "\n",
    "# for verb_phrase in verb_phrases:\n",
    "#     print(verb_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using show description along with episode description (restricting shows that have ten or more episodes)\n",
    "# descriptions = [\" \".join([phrase for phrase in entry]) for entry in descriptions_aggregated if len(entry) > 10]\n",
    "# cleaned_descriptions = clean_descriptions(descriptions)\n",
    "\n",
    "## using show descriptions only\n",
    "# cleaned_descriptions = clean_descriptions(list(show_descriptions.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippet to process named entities\n",
    "def process_nes(docs):\n",
    "    final = []\n",
    "    for val in docs:\n",
    "        if val:\n",
    "            phrases = []\n",
    "            if len(val) > 1:\n",
    "                for phrase in list(val):\n",
    "                    phrase = re.sub(r'www\\.[a-z0-9A-Z.]*', '', str(phrase))\n",
    "                    phrase = re.sub(r'https://[a-z0-9A-Z.]*', '', phrase)\n",
    "                    phrase = re.sub(r'http://[a-z0-9A-Z.]*', '', phrase)\n",
    "                    phrase = re.sub(r'[^0-9a-zA-Z\\\" \"]+', '', phrase)\n",
    "                    phrase = re.sub(r\"  \", \" \", phrase)\n",
    "                    phrase = re.sub(r\"   \", \" \", phrase)\n",
    "                    phrase = re.sub(r\"   \", \" \", phrase)\n",
    "                    #phrases.append(phrase)\n",
    "                    #phrases.append(\"_\".join(phrase.split(\" \")))\n",
    "                    if phrase!=\"\":final.append(phrase)\n",
    "            else:\n",
    "                phrase = re.sub(r'www\\.[a-z0-9A-Z.]*', '', str(val[0]))\n",
    "                phrase = re.sub(r'https://[a-z0-9A-Z.]*', '', phrase)\n",
    "                phrase = re.sub(r'http://[a-z0-9A-Z.]*', '', phrase)\n",
    "                phrase = re.sub(r'[^0-9a-zA-Z\\\" \"]+', '', phrase)\n",
    "                phrase = re.sub(r\"  \", \" \", phrase)\n",
    "                phrase = re.sub(r\"   \", \" \", phrase)\n",
    "                phrase = re.sub(r\"   \", \" \", phrase)\n",
    "                #phrases.append(phrase)\n",
    "                #phrases.append(\"_\".join(phrase.split(\" \")))\n",
    "                if phrase!=\"\":final.append(phrase)\n",
    "                #(\" \".join([item for item in phrases if not item.isdigit()]))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nes = [list(val) for key,val in pos_show_descriptions[\"NE\"].items()]\n",
    "nes = process_nes(nes)\n",
    "nes_descriptions = process_nes([[description] for description in show_descriptions.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nes_tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                       max_features = 1000,\n",
    "                                       min_df = 1,\n",
    "                                       max_df = 0.5,\n",
    "                                       ngram_range = (2,3),\n",
    "                                       stop_words = STOP_WORDS.union({'ll','ve','pron','okay','oh','like','know','yeah','yea',\\\n",
    "                                                                  'yep',\"like like\",\"oh like\",\"yeah like\",\"yeah yeah\",\"oh okay\",\\\n",
    "                                                                  \"wow\",\"podcast\",\"support\",\"talk\",\"talking\",\"people\",\"shit\",\\\n",
    "                                                                 \"stuff\",\"things\",\"join\",\"week\",\"weekly\"}))\n",
    "\n",
    "nes_tfidf_vectorizer.fit_transform(nes)\n",
    "nes_features = nes_tfidf_vectorizer.get_feature_names() #contains the same information as mapping, as a list\n",
    "mapping = nes_tfidf_vectorizer.vocabulary_ #contains the same information as features, as a dictiionary \n",
    "nes_matrix = nes_tfidf_vectorizer.transform(nes_descriptions)\n",
    "nes_matrix_dense = nes_matrix.todense()\n",
    "matrix_denselist = nes_matrix_dense.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nes_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For noun-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_stop_nouns = [\" \".join(val) for key,val in pos_show_descriptions[\"NOUN\"].items()]\n",
    "# cleaned_noun_descriptions = clean_descriptions(non_stop_nouns)\n",
    "# cleaned_descriptions = clean_descriptions(list(show_descriptions.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                                   max_features = 1000,\n",
    "                                   min_df = 5,\n",
    "                                   max_df = 0.9,\n",
    "                                   ngram_range = (1,1),\n",
    "                                   stop_words = STOP_WORDS.union({'ll','ve','pron','okay','oh','like','know','yeah','yea',\\\n",
    "                                                                  'yep',\"like like\",\"oh like\",\"yeah like\",\"yeah yeah\",\"oh okay\",\\\n",
    "                                                                  \"wow\",\"podcast\",\"support\",\"talk\",\"talking\",\"people\",\"shit\",\\\n",
    "                                                                 \"stuff\",\"things\",\"join\",\"week\",\"weekly\"}))\n",
    "\n",
    "tfidf_vectorizer.fit_transform(list(cleaned_noun_descriptions))\n",
    "features = tfidf_vectorizer.get_feature_names() #contains the same information as mapping, as a list\n",
    "mapping = tfidf_vectorizer.vocabulary_ #contains the same information as features, as a dictiionary \n",
    "matrix = tfidf_vectorizer.transform(cleaned_descriptions)\n",
    "matrix_dense = matrix.todense()\n",
    "matrix_denselist = matrix_dense.tolist()\n",
    "\n",
    "all_keywords = []\n",
    "\n",
    "for transcript in matrix_denselist:\n",
    "    k = 0\n",
    "    keywords = []\n",
    "    for word in transcript:\n",
    "        if word > 0:\n",
    "            keywords.append(get_key(mapping,k))\n",
    "        k += 1\n",
    "    all_keywords.append(keywords)\n",
    "\n",
    "number_of_clusters = 30\n",
    "\n",
    "model = KMeans(n_clusters = number_of_clusters, init = \"k-means++\", max_iter = 100,n_init=1)\n",
    "\n",
    "model.fit(matrix)\n",
    "\n",
    "order_centers = model.cluster_centers_.argsort()[:,::-1]\n",
    "\n",
    "with open(\"show_descriptions_nouns_tfidf_kemans_clusters.txt\", \"w\") as f:\n",
    "    for i in range(number_of_clusters):\n",
    "        f.write(f'Cluster {i}')\n",
    "        f.write('\\n')\n",
    "        for ind in order_centers[i, :10]:\n",
    "            f.write(' %s' %get_key(mapping,ind),)\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = {}\n",
    "\n",
    "for transcript,name in zip(matrix_denselist,list(show_descriptions.keys())):\n",
    "    k = 0\n",
    "    keywords = []\n",
    "    for word in transcript:\n",
    "        if word > 0:\n",
    "            keywords.append(get_key(mapping,k))\n",
    "        k += 1\n",
    "    all_keywords[name] = keywords\n",
    "    \n",
    "# all_keywords_v2 = []\n",
    "\n",
    "# for transcript in matrix_denselist:\n",
    "#     k = 0\n",
    "#     keywords = []\n",
    "#     for word in transcript:\n",
    "#         if word > 0:\n",
    "#             keywords.append(features[k])\n",
    "#         k += 1\n",
    "#     all_keywords_v2.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "keyword_counts_show_descriptions = pd.DataFrame(matrix.toarray(),columns = features,index=list(show_descriptions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_counts_show_descriptions[keyword_counts_show_descriptions['music'] > 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for keyword in [features[k] for k in order_centers[0][1:2]]:\n",
    "#     print(list(keyword_counts_show_descriptions[keyword_counts_show_descriptions[keyword] > 0].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the show description to cluster association \n",
    "\n",
    "show_vs_cluster = {}\n",
    "for name in list(show_descriptions.keys()):\n",
    "    for k in range(number_of_clusters):\n",
    "        n = set(all_keywords[name]).intersection(set([features[j] for j in order_centers[k][:20]]))\n",
    "        if len(n) >= 3:\n",
    "            if name not in show_vs_cluster:\n",
    "                show_vs_cluster[name] = [k]\n",
    "            else:\n",
    "                show_vs_cluster[name].extend([k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
